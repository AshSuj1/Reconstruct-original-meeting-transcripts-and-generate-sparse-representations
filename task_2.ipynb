{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Sparse Representations \n",
    "\n",
    "Version: 2.0\n",
    "\n",
    "Environment: Python 3.6.3 and Anaconda 4.3.0 (64-bit)\n",
    "\n",
    "Libraries used: \n",
    "* re (for regular expression, included in Anaconda Python 3.6)\n",
    "* os to join and load all files\n",
    "* nltk to process unigrams\n",
    "* nltk probability to find frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import multiprocessing as mp\n",
    "from nltk.probability import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenize\n",
    "\n",
    "In this section, we will perform the following tasks:\n",
    "* load data\n",
    "* instantiate tokenizer, the regex was provided\n",
    "* open stop words list\n",
    "* examine content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer string provided with assignment\n",
    "tokenizer = RegexpTokenizer(r\"\\w+(?:[-']\\w+)?\")\n",
    "# load all text files and append to a list\n",
    "#file_Path = \"C:\\\\Users\\\\AshSu\\\\Downloads\\\\meeting_transcripts_student\\\\meeting_transcripts_student\\\\txt_files\"\n",
    "file_Path = \"./txt_files\"\n",
    "# list to append each text to\n",
    "All_texts = []\n",
    "\n",
    "# function provided in tutorials\n",
    "for txt_object in os.listdir(file_Path):\n",
    "    file = os.path.join(file_Path, txt_object)\n",
    "    with open(file, 'r') as Text_File:\n",
    "        # load all lines\n",
    "        text = Text_File.readlines()\n",
    "        All_texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open Stop Words file\n",
    "with open('stopwords_en.txt','r') as Stopword_file:\n",
    "    stop_Words = Stopword_file.read()\n",
    "    \n",
    "stop_Words = stop_Words.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a few lines in the All_text list to see that it worked properly\n",
    "print(All_texts[50:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how do the topic breaks appear in our text files\n",
    "for text_file in All_texts:\n",
    "    for word in text_file:\n",
    "        if word.startswith(\"**********\"):\n",
    "            print([word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to capture all breaks in the topic\n",
    "# there are two conditions in which the break in topic occurs appears\n",
    "# simply as 10 asterix\n",
    "for text_file in All_texts:\n",
    "    for word in text_file:\n",
    "        if word == '**********':\n",
    "            print([word])\n",
    "        # or joined by a new line \n",
    "        if word == '**********\\n':\n",
    "            print([word[0:10]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenize\n",
    "\n",
    "In this section, we will perform the following tasks:\n",
    "* apply the tokenizer provided\n",
    "* examine content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenize_text(List_oftextfiles):\n",
    "    # make a list of all the tokens, the idea is that each line in each text becomes its own list of tokens\n",
    "    # such as the example provided in the lecture\n",
    "    Unigrams = []\n",
    "    # iterate through all the text files that have been joined together in a list\n",
    "    for text_file in List_oftextfiles:\n",
    "        # build a new list to collect all the individual tokens\n",
    "        Wordtokens = []\n",
    "        for word in text_file:\n",
    "            # we want to capture all breaks in the topic as well\n",
    "            # there are two conditions in which the segment appears\n",
    "            # simply as 10 asterix\n",
    "            if word == '**********':\n",
    "                Wordtokens.append([word])\n",
    "            # or joined by a new line if there is a corresponding segment break\n",
    "            if word == '**********\\n':\n",
    "                # we want to skip the \"\\n\", so we only append the asterix\n",
    "                Wordtokens.append([word[0:10]])\n",
    "            else:\n",
    "                # since the words must be converted into lower case we can convert each words within the tokenizer function\n",
    "                token = tokenizer.tokenize(word.lower())\n",
    "                Wordtokens.append(token)\n",
    "        Unigrams.append(Wordtokens)\n",
    "        \n",
    "\n",
    "    return Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokenize_text(All_texts[50:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text =Tokenize_text(All_texts)\n",
    "\n",
    "for text in textList:\n",
    "        for row in text:\n",
    "            print(row)\n",
    "            for word in row:\n",
    "                print(word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Remove all Stop words \n",
    "\n",
    "In this section, we will perform the following tasks:\n",
    "* build a function that removes stop words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_stopwords(texts):\n",
    "    # intantiate a list that will contain other lists, each list being a text, which contains other lists, each list here being\n",
    "    # a particular sentence\n",
    "    Text_withoutSW = []\n",
    "    # use a for loop to iterate over the tokenized texts\n",
    "    for text in texts:\n",
    "        # instantiate another list, this will be the list that contains the separated lines\n",
    "        TokensList_withoutSW = []\n",
    "        for line in text:\n",
    "            # borrowed from lecture material \"Exploring Pre-Processed text and Generating Features \"\n",
    "            wordsSet = [word for word in line if word not in stop_Words]\n",
    "            # make sure wordsSet is not empty\n",
    "            if wordsSet:\n",
    "                # then append list TokensList_withoutSW list\n",
    "                TokensList_withoutSW.append(wordsSet)\n",
    "        # then append this list for each text\n",
    "        Text_withoutSW.append(TokensList_withoutSW)\n",
    "        \n",
    "    return Text_withoutSW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Find words that have a frequency greater than 132 \n",
    "\n",
    "In this section, we will perform the following tasks:\n",
    "* build a function that collects all the words that appear more than 132 times\n",
    "* build another function that removes these words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using set method find all unique words in the text overall\n",
    "def Unique_words(Alltext):\n",
    "    One_text = text_stopwords(Tokenize_text(Alltext))\n",
    "    New_set = set()\n",
    "    for text in One_text:\n",
    "        for line in text:\n",
    "            for word in line:\n",
    "                if word is \"**********\":\n",
    "                    pass\n",
    "                else:\n",
    "                    New_set.add(word)\n",
    "    \n",
    "    return New_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findfrequency(All_text, Unique):\n",
    "    Common_words = []\n",
    "    All_possiblewords = []\n",
    "    for text in All_text:\n",
    "        Set_2 = set()\n",
    "        for row in text:\n",
    "            for word in row:\n",
    "                if word =='**********':\n",
    "                    pass\n",
    "                else:\n",
    "                    Set_2.add(word)\n",
    "        All_possiblewords.append(Set_2)\n",
    "    for word in Unique:\n",
    "        x = 0\n",
    "        for List in All_possiblewords:\n",
    "            if word in List:\n",
    "                x += 1\n",
    "                \n",
    "        if x > 132:\n",
    "            Common_words.append(word)\n",
    "    return(Common_words)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Common_Words = findfrequency(text_stopwords(Tokenize_text(All_texts)), Unique_words(All_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Common_Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blah_blah = All_texts\n",
    "for x, text in enumerate(blah_blah):\n",
    "    for ID, line in enumerate(text):\n",
    "        print(index, line)\n",
    "        for index, word in enumerate(line):\n",
    "            print(x, ID, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index(AllText):\n",
    "    some_list = []\n",
    "    Common_Words = findfrequency(text_stopwords(Tokenize_text(AllText)), Unique_words(AllText))\n",
    "\n",
    "    for com_word in Common_Words:\n",
    "        \n",
    "        for idx,text in enumerate(AllText):\n",
    "            for idy, line in enumerate(text):\n",
    "                for idz, word in enumerate(line):\n",
    "                    if com_word == word:\n",
    "                        some_list.append([idx][idy][idz])\n",
    "        \n",
    "                        \n",
    "    return some_list\n",
    "\n",
    "\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Common_Words = findfrequency(text_stopwords(Tokenize_text(All_texts)), Unique_words(All_texts))\n",
    "some_list = []\n",
    "for com_word in Common_Words:\n",
    "    for idx,text in enumerate(All_texts):\n",
    "        for idy, line in enumerate(text):\n",
    "            for idz, word in enumerate(line):\n",
    "                if com_word == word:\n",
    "                    print(idx,idy,idz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(some_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_otherlist = find_index(All_texts)\n",
    "    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Vocabulary(Unique):\n",
    "    unique = list(Unique)\n",
    "    for x in range(len(unique)):\n",
    "        unique[x] = unique[x] + \":\" + str(x)\n",
    "\n",
    "    Vocab_String = '\\n'.join(unique)\n",
    "    \n",
    "    return Vocab_String\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok = Unique_words(All_texts)\n",
    "Vocab = Vocabulary(ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('vocab.txt', 'w') as VocabFile:\n",
    "    VocabFile.write(Vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. References\n",
    "* Bird, S., Klein, E., & Loper, E. (2009). Natural language processing with Python:. Beijing: OReilly.\n",
    "* Is there a more Pythonic way to prevent adding a duplicate to a list? (n.d.). Retrieved from https://stackoverflow.com/questions/19834806/is-there-a-more-pythonic-way-to-prevent-adding-a-duplicate-to-a-list\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
